{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to","text":""},{"location":"#scikit-eo-a-python-package-for-remote-sensing-tools","title":"scikit-eo: A Python package for Remote Sensing Tools","text":""},{"location":"#links-of-interest","title":"Links of interest:","text":"<ul> <li>GitHub repo: https://github.com/yotarazona/scikit-eo</li> <li>Documentation: https://yotarazona.github.io/scikit-eo/</li> <li>PyPI: https://pypi.org/project/scikeo/</li> <li>Notebooks examples: https://github.com/yotarazona/scikit-eo/tree/main/examples</li> <li>Google Colab examples: https://github.com/yotarazona/scikit-eo/tree/main/examples</li> <li>Free software: Apache 2.0</li> <li>Tutorials: step by step: https://yotarazona.github.io/scikit-eo/tutorials/</li> </ul>"},{"location":"#introduction","title":"Introduction","text":"<p>Nowadays, remotely sensed data has increased dramatically. Microwaves and optical images with different spatial and temporal resolutions are available and are used to monitor a variety of environmental issues such as deforestation, land degradation, land use and land cover change, among others. Although there are efforts (i.e., Python packages, forums, communities, etc.) to make available line-of-code tools for pre-processing, processing and analysis of satellite imagery, there is still a gap that needs to be filled. In other words, too much time is still spent by many users developing Python lines of code. Algorithms for mapping land degradation through a linear trend of vegetation indices, fusion optical and radar images to classify vegetation cover, and calibration of machine learning algorithms, among others, are not available yet.</p> <p>Therefore, scikit-eo is a Python package that provides tools for remote sensing. This package was developed to fill the gaps in remotely sensed data processing tools. Most of the tools are based on scientific publications, and others are useful algorithms that will allow processing to be done in a few lines of code. With these tools, the user will be able to invest time in analyzing the results of their data and not spend time on elaborating lines of code, which can sometimes be stressful.</p>"},{"location":"#audience","title":"Audience","text":"<p>Scikit-eo is a versatile Python package designed to cover a wide range of users, including students, professionals of remote sensing, researchers of environmental analysis, and organizations looking for satellite image analysis. Its comprehensive features make it well-suited for various applications, such as university teaching, that include technical and practical sessions, and cutting-edge research using the most recent machine learning and deep learning techniques applied to the field of remote sensing. Whether the user are students seeking to get insights from a satellite image analysis or a experienced researcher looking for advanced tools, scikit-eo offers a valuable resource to support the most valuable methods for environmental studies.</p>"},{"location":"#tools-for-remote-sensing","title":"Tools for Remote Sensing","text":"Name of functions/classes Description <code>mla</code> Machine Learning (Random Forest, Support Vector Machine, Decition Tree, Naive Bayes, Neural Network, etc.) <code>calmla</code> Calibrating supervised classification in Remote Sensing (e.g., Monte Carlo Cross-Validation, Leave-One-Out Cross-Validation, etc.) <code>confintervalML</code> Information of confusion matrix by proportions of area, overall accuracy, user's accuracy with confidence interval and estimated area with confidence interval as well. <code>rkmeans</code> K-means classification <code>calkmeans</code> This function allows to calibrate the kmeans algorithm. It is possible to obtain the best k value and the best embedded algorithm in kmeans. <code>pca</code> Principal Components Analysis <code>atmosCorr</code> Atmospheric Correction of satellite imagery <code>deepLearning</code> Deep Learning algorithms <code>linearTrend</code> Linear trend is useful for mapping forest degradation or land degradation <code>fusionrs</code> This algorithm allows to fuse images coming from different spectral sensors (e.g., optical-optical, optical and SAR or SAR-SAR). Among many of the qualities of this function, it is possible to obtain the contribution (%) of each variable in the fused image <code>sma</code> Spectral Mixture Analysis - Classification sup-pixel <code>tassCap</code> The Tasseled-Cap Transformation <p>You will find more algorithms!.</p>"},{"location":"#installation","title":"Installation","text":"<p>To use scikit-eo it is necessary to install it. There are two options:</p>"},{"location":"#1-from-pypi","title":"1. From PyPI","text":"<pre><code>pip install scikeo\n</code></pre>"},{"location":"#2-installing-from-source","title":"2. Installing from source","text":"<p>It is also possible to install the latest development version directly from the GitHub repository with:</p> <pre><code>pip install git+https://github.com/yotarazona/scikit-eo\n</code></pre>"},{"location":"atmosCorr/","title":"atmosCorr module","text":""},{"location":"atmosCorr/#module-atmoscorr","title":"module <code>atmosCorr</code>","text":""},{"location":"atmosCorr/#class-atmoscorr","title":"class <code>atmosCorr</code>","text":"<p>Atmospheric Correction in Optical domain </p> <p></p>"},{"location":"atmosCorr/#method-__init__","title":"method <code>__init__</code>","text":"<pre><code>__init__(path, nodata=-99999)\n</code></pre> <p>Parameter: </p> <p>path: String. The folder in which the satellite bands are located. This images could be Landsat  Collection 2 Level-1. For example: path = r'/folder/image/raster'.  </p> <p>nodata: The NoData value to replace with -99999. </p> <p></p>"},{"location":"atmosCorr/#method-dos","title":"method <code>DOS</code>","text":"<pre><code>DOS(sat='LC08', mindn=None)\n</code></pre> <p>The Dark Object Subtraction Method was proposed by Chavez (1988). This image-based  atmospheric correction method considers absolutely critical and valid the existence  of a dark object in the scene, which is used in the selection of a minimum value in  the haze correction. The most valid dark objects in this kind of correction are areas  totally shaded or otherwise areas representing dark water bodies. </p> <p>Parameters:</p> <ul> <li> <p><code>sat</code>:  Type of Satellite. It could be Landsat-5 TM, Landsat-8 OLI or Landsat-9 OLI-2. </p> </li> <li> <p><code>mindn</code>:  Min of digital number for each band in a list. </p> </li> </ul> <p>Return:  An array with Surface Reflectance values with 3d, i.e. (rows, cols, bands). </p> <p>References: </p> <p>Chavez, P.S. (1988). An Improved Dark-Object Subtraction Technique for Atmospheric  Scattering Correction of Multispectral Data. Remote Sensing of Envrironment, 24(3), 459-479. </p> <p></p>"},{"location":"atmosCorr/#method-rad","title":"method <code>RAD</code>","text":"<pre><code>RAD(sat='LC08')\n</code></pre> <p>Conversion to TOA Radiance. Landsat Level-1 data can be converted to TOA spectral radiance  using the radiance rescaling factors in the MTL file: </p> <p>L\u03bb = MLQcal + AL  </p> <p>where: </p> <p>L\u03bb = TOA spectral radiance (Watts/(m2srad\u03bcm)) ML = Band-specific multiplicative rescaling factor from the metadata (RADIANCE_MULT_BAND_x, where x is the band number) AL = Band-specific additive rescaling factor from the metadata (RADIANCE_ADD_BAND_x, where x is the band number) Qcal =  Quantized and calibrated standard product pixel values (DN)  </p> <p>Parameters:</p> <ul> <li><code>sat</code>:  Type of Satellite. It could be Landsat-5 TM, Landsat-8 OLI or Landsat-9 OLI-2. </li> </ul> <p>Return:  An array with radiance values with 3d, i.e. (rows, cols, bands). </p> <p></p>"},{"location":"atmosCorr/#method-toa","title":"method <code>TOA</code>","text":"<pre><code>TOA(sat='LC08')\n</code></pre> <p>A reduction in scene-to-scene variability can be achieved by converting the at-sensor  spectral radiance to exoatmospheric TOA reflectance, also known as in-band planetary albedo. </p> <p>Equation to obtain TOA reflectance: </p> <p>\u03c1\u03bb\u2032 = M\u03c1*DN + A\u03c1 </p> <p>\u03c1\u03bb = \u03c1\u03bb\u2032/sin(theta) </p> <p>Parameters:</p> <ul> <li><code>sat</code>:  Type of Satellite. It could be Landsat-5 TM, Landsat-8 OLI or Landsat-9 OLI-2. </li> </ul> <p>Return:  An array with TOA values with 3d, i.e. (rows, cols, bands). </p> <p>This file was automatically generated via lazydocs.</p>"},{"location":"calkmeans/","title":"calkmeans module","text":""},{"location":"calkmeans/#module-calkmeans","title":"module <code>calkmeans</code>","text":""},{"location":"calkmeans/#function-calkmeans","title":"function <code>calkmeans</code>","text":"<pre><code>calkmeans(\n    image,\n    k=None,\n    algo=('auto', 'elkan'),\n    max_iter=300,\n    n_iter=10,\n    nodata=-99999,\n    **kwargs\n)\n</code></pre> <p>Calibrating kmeans </p> <p>This function allows to calibrate the kmeans algorithm. It is possible to obtain the best 'k' value and the best embedded algorithm in KMmeans.  </p> <p>Parameters:</p> <ul> <li> <p><code>image</code>:  Optical images. It must be rasterio.io.DatasetReader with 3d.  </p> </li> <li> <p><code>k</code>:  k This argument is None when the objective is to obtain the best 'k' value.   f the objective is to select the best algorithm embedded in kmeans, please specify a 'k' value. </p> </li> <li> <p><code>max_iter</code>:  The maximum number of iterations allowed. Strictly related to KMeans. Please see </p> </li> <li> <p><code>https</code>: //scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html </p> </li> <li> <p><code>algo</code>:  It can be \"auto\" and 'elkan'. \"auto\" and \"full\" are deprecated and they will be   removed in Scikit-Learn 1.3. They are both aliases for \"lloyd\".  </p> </li> <li> <p><code>Changed in version 1.1</code>:  Renamed \u201cfull\u201d to \u201clloyd\u201d, and deprecated \u201cauto\u201d and \u201cfull\u201d.   Changed \u201cauto\u201d to use \u201clloyd\u201d instead of \u201celkan\u201d.  </p> </li> <li> <p><code>n_iter</code>:  Iterations number to obtain the best 'k' value. 'n_iter' must be greater than the   number of classes expected to be obtained in the classification. Default is 10.  </p> </li> <li> <p><code>nodata</code>:  The NoData value to replace with -99999.   </p> </li> <li> <p><code>**kwargs</code>:  These will be passed to scikit-learn KMeans, please see full lists at: </p> </li> <li><code>https</code>: //scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html. </li> </ul> <p>Return: Labels of classification as numpy object with 2d. </p> <p>Note:</p> <p>If the idea is to find the optimal value of 'k' (clusters or classes), k = None as an argument of the function must be put, because the function find 'k' for which the intra-class inertia is stabilized. If the 'k' value is known and the idea is to find the best algorithm embedded in kmeans (that maximizes inter-class distances), k = n, which 'n' is a specific class number, must be put. It can be greater than or equal to 0.  </p> <p>This file was automatically generated via lazydocs.</p>"},{"location":"calmla/","title":"calmla module","text":""},{"location":"calmla/#module-calmla","title":"module <code>calmla</code>","text":""},{"location":"calmla/#class-calmla","title":"class <code>calmla</code>","text":"<p>Calibrating supervised classification in Remote Sensing </p> <p>This module allows to calibrate supervised classification in satellite images through various algorithms and using approaches such as Set-Approach,  Leave-One-Out Cross-Validation (LOOCV), Cross-Validation (k-fold) and  Monte Carlo Cross-Validation (MCCV) </p> <p></p>"},{"location":"calmla/#method-__init__","title":"method <code>__init__</code>","text":"<pre><code>__init__(endmembers)\n</code></pre> <p>Parameter: </p> <p>endmembers: Endmembers must be a matrix (numpy.ndarray) and with more than one endmember.   Rows represent the endmembers and columns represent the spectral bands.  The number of bands must be equal to the number of endmembers.  E.g. an image with 6 bands, endmembers dimension should be $n*6$, where $n$   is rows with the number of endmembers and 6 is the number of bands   (should be equal).  In addition, Endmembers must have a field (type int or float) with the names   of classes to be predicted. </p> <p>References: </p> <p>Tarazona, Y., Zabala, A., Pons, X., Broquetas, A., Nowosad, J., and Zurqani, H.A.   Fusing Landsat and SAR data for mapping tropical deforestation through machine learning   classification and the PVts-\u03b2 non-seasonal detection approach, Canadian Journal of Remote   Sensing., vol. 47, no. 5, pp. 677\u2013696, Sep. 2021. </p> <p></p>"},{"location":"calmla/#method-cv","title":"method <code>CV</code>","text":"<pre><code>CV(\n    split_data,\n    models=('svm', 'dt'),\n    k=5,\n    n_iter=10,\n    random_state=None,\n    **kwargs\n)\n</code></pre> <p>This module allows to calibrate supervised classification in satellite images  through various algorithms and using Cross-Validation. </p> <p>Parameters:</p> <ul> <li> <p><code>split_data</code>:  A dictionary obtained from the splitData method of this package. </p> </li> <li> <p><code>models</code>:  Models to be used such as Support Vector Machine ('svm'), Decision Tree ('dt'), Random Forest ('rf'), Naive Bayes ('nb') and Neural Networks ('nn'). This parameter can be passed like models = ('svm', 'dt', 'rf', 'nb', 'nn'). </p> </li> <li> <p><code>cv</code>:  For splitting samples into two subsets, i.e. training data and  for testing data. Following Leave One Out Cross-Validation. </p> </li> <li> <p><code>n_iter</code>:  Number of iterations, i.e number of times the analysis is executed. </p> </li> <li> <p><code>**kwargs</code>:  These will be passed to SVM, DT, RF, NB and NN, please see full lists at: </p> </li> <li><code>https</code>: //scikit-learn.org/stable/supervised_learning.html#supervised-learning </li> </ul> <p>Return:   A graphic with errors for each machine learning algorithms. </p> <p></p>"},{"location":"calmla/#method-loocv","title":"method <code>LOOCV</code>","text":"<pre><code>LOOCV(split_data, models=('svm', 'dt'), cv=LeaveOneOut(), n_iter=10, **kwargs)\n</code></pre> <p>This module allows to calibrate supervised classification in satellite images  through various algorithms and using Leave One Out Cross-Validation. </p> <p>Parameters:</p> <ul> <li> <p><code>split_data</code>:  A dictionary obtained from the splitData method of this package. </p> </li> <li> <p><code>models</code>:  Models to be used such as Support Vector Machine ('svm'), Decision Tree ('dt'), Random Forest ('rf'), Naive Bayes ('nb') and Neural Networks ('nn'). This parameter can be passed like models = ('svm', 'dt', 'rf', 'nb', 'nn'). </p> </li> <li> <p><code>cv</code>:  For splitting samples into two subsets, i.e. training data and  for testing data. Following Leave One Out Cross-Validation. </p> </li> <li> <p><code>n_iter</code>:  Number of iterations, i.e number of times the analysis is executed. </p> </li> <li> <p><code>**kwargs</code>:  These will be passed to SVM, DT, RF, NB and NN, please see full lists at: </p> </li> <li><code>https</code>: //scikit-learn.org/stable/supervised_learning.html#supervised-learning </li> </ul> <p>Return:   A graphic with errors for each machine learning algorithms.  </p> <p></p>"},{"location":"calmla/#method-mccv","title":"method <code>MCCV</code>","text":"<pre><code>MCCV(\n    split_data,\n    models='svm',\n    train_size=0.5,\n    n_splits=5,\n    n_iter=10,\n    random_state=None,\n    **kwargs\n)\n</code></pre> <p>This module allows to calibrate supervised classification in satellite images  through various algorithms and using Cross-Validation. </p> <p>Parameters:</p> <ul> <li> <p><code>split_data</code>:  A dictionary obtained from the splitData method of this package. </p> </li> <li> <p><code>models</code>:  Models to be used such as Support Vector Machine ('svm'), Decision Tree ('dt'), Random Forest ('rf'), Naive Bayes ('nb') and Neural Networks ('nn'). This parameter can be passed like models = ('svm', 'dt', 'rf', 'nb', 'nn'). </p> </li> <li> <p><code>cv</code>:  For splitting samples into two subsets, i.e. training data and  for testing data. Following Leave One Out Cross-Validation. </p> </li> <li> <p><code>n_iter</code>:  Number of iterations, i.e number of times the analysis is executed. </p> </li> <li> <p><code>**kwargs</code>:  These will be passed to SVM, DT, RF, NB and NN, please see full lists at: </p> </li> <li><code>https</code>: //scikit-learn.org/stable/supervised_learning.html#supervised-learning </li> </ul> <p>Return:   A graphic with errors for each machine learning algorithms.  </p> <p></p>"},{"location":"calmla/#method-sa","title":"method <code>SA</code>","text":"<pre><code>SA(split_data, models=('svm', 'dt', 'rf'), train_size=0.5, n_iter=10, **kwargs)\n</code></pre> <p>This module allows to calibrate supervised classification in satellite images  through various algorithms and using approaches such as Set-Approach. </p> <p>Parameters:</p> <ul> <li> <p><code>split_data</code>:  A dictionary obtained from the splitData method of this package. </p> </li> <li> <p><code>models</code>:  Models to be used such as Support Vector Machine ('svm'), Decision Tree ('dt'), Random Forest ('rf'), Naive Bayes ('nb') and Neural Networks ('nn'). This parameter can be passed like models = ('svm', 'dt', 'rf', 'nb', 'nn'). </p> </li> <li> <p><code>train_size</code>:  For splitting samples into two subsets, i.e. training data and  for testing data. </p> </li> <li> <p><code>n_iter</code>:  Number of iterations, i.e number of times the analysis is executed. </p> </li> <li> <p><code>**kwargs</code>:  These will be passed to SVM, DT, RF, NB and NN, please see full lists at: </p> </li> <li><code>https</code>: //scikit-learn.org/stable/supervised_learning.html#supervised-learning </li> </ul> <p>Return:   A graphic with errors for each machine learning algorithms. </p> <p></p>"},{"location":"calmla/#method-splitdata","title":"method <code>splitData</code>","text":"<pre><code>splitData(random_state=None)\n</code></pre> <p>This method is to separate the dataset in predictor variables and the variable  to be predicted </p> <p>Parameter:  </p> <p>self: Attributes of class calmla. </p> <p>Return:  A dictionary with X and y. </p> <p>This file was automatically generated via lazydocs.</p>"},{"location":"contributing/","title":"Contributing","text":"<p>Contributions are welcome, and they are greatly appreciated! Every little bit helps, and credit will always be given.</p> <p>You can contribute in many ways:</p>"},{"location":"contributing/#types-of-contributions","title":"Types of Contributions","text":""},{"location":"contributing/#report-bugs","title":"Report Bugs","text":"<p>Report bugs at https://github.com/ytarazona/scikit-eo/issues.</p> <p>If you are reporting a bug, please include:</p> <ul> <li>Your operating system name and version.</li> <li>Any details about your local setup that might be helpful in troubleshooting.</li> <li>Detailed steps to reproduce the bug.</li> </ul>"},{"location":"contributing/#fix-bugs","title":"Fix Bugs","text":"<p>Look through the GitHub issues for bugs. Anything tagged with <code>bug</code> and <code>help wanted</code> is open to whoever wants to implement it.</p>"},{"location":"contributing/#implement-features","title":"Implement Features","text":"<p>Look through the GitHub issues for features. Anything tagged with <code>enhancement</code> and <code>help wanted</code> is open to whoever wants to implement it.</p>"},{"location":"contributing/#write-documentation","title":"Write Documentation","text":"<p>scikit-eo could always use more documentation, whether as part of the official scikit-eo docs, in docstrings, or even on the web in blog posts, articles, and such.</p>"},{"location":"contributing/#submit-feedback","title":"Submit Feedback","text":"<p>The best way to send feedback is to file an issue at https://github.com/ytarazona/scikit-eo/issues.</p> <p>If you are proposing a feature:</p> <ul> <li>Explain in detail how it would work.</li> <li>Keep the scope as narrow as possible, to make it easier to implement.</li> <li>Remember that this is a volunteer-driven project, and that contributions are welcome :)</li> </ul>"},{"location":"contributing/#get-started","title":"Get Started!","text":"<p>Ready to contribute? Here's how to set up scikit-eo for local development.</p> <ol> <li> <p>Fork the scikit-eo repo on GitHub.</p> </li> <li> <p>Clone your fork locally:</p> <pre><code>$ git clone git@github.com:your_name_here/scikit-eo.git\n</code></pre> </li> <li> <p>Install your local copy into a virtualenv. Assuming you have     virtualenvwrapper installed, this is how you set up your fork for     local development:</p> <pre><code>$ mkvirtualenv scikit-eo\n$ cd scikit-eo/\n$ python setup.py develop\n</code></pre> </li> <li> <p>Create a branch for local development:</p> <pre><code>$ git checkout -b name-of-your-bugfix-or-feature\n</code></pre> <p>Now you can make your changes locally.</p> </li> <li> <p>When you're done making changes, check that your changes pass flake8     and the tests, including testing other Python versions with tox:</p> <pre><code>$ flake8 scikit-eo tests\n$ python setup.py test or pytest\n$ tox\n</code></pre> <p>To get flake8 and tox, just pip install them into your virtualenv.</p> </li> <li> <p>Commit your changes and push your branch to GitHub:</p> <pre><code>$ git add .\n$ git commit -m \"Your detailed description of your changes.\"\n$ git push origin name-of-your-bugfix-or-feature\n</code></pre> </li> <li> <p>Submit a pull request through the GitHub website.</p> </li> </ol>"},{"location":"contributing/#pull-request-guidelines","title":"Pull Request Guidelines","text":"<p>Before you submit a pull request, check that it meets these guidelines:</p> <ol> <li>The pull request should include tests.</li> <li>If the pull request adds functionality, the docs should be updated.     Put your new functionality into a function with a docstring, and add     the feature to the list in README.rst.</li> <li>The pull request should work for Python 3.5, 3.6, 3.7 and 3.8, and     for PyPy. Check https://github.com/ytarazona/scikit-eo/pull_requests and make sure that the tests pass for all     supported Python versions.</li> </ol>"},{"location":"contributors/","title":"Contributors","text":""},{"location":"contributors/#maintainers-and-contributors","title":"Maintainers and contributors","text":"<p>Scikit-eo is maintained by a core group of Python enthusiasts working in various international universities such as Pontificia Universidad Cat\u00f3lica del Per\u00fa (Peru), University of Coimbra (Portugal), Adam Mickiewicz University in Pozna\u0144 (Poland) and The school of geography and sustainable development, University of St Andrews (UK).</p> <ul> <li>Yonatan Tarazona</li> <li>Jakub Nowosad</li> <li>Fernando Benitez-Paez</li> <li>David Montero Loaiza</li> </ul>"},{"location":"contributors/#contributors-and-collaborators","title":"Contributors and collaborators","text":"<p>We want to thank the following people for collaborating to the scikeo package.</p> <ul> <li>Fabian Drenkhan</li> <li>Mart\u00edn E. Timan\u00e1</li> </ul> <p>List to be added</p>"},{"location":"deeplearning/","title":"deeplearning module","text":""},{"location":"deeplearning/#module-deeplearning","title":"module <code>deeplearning</code>","text":""},{"location":"deeplearning/#class-dl","title":"class <code>DL</code>","text":"<p>Deep Learning classification in Remote Sensing </p> <p></p>"},{"location":"deeplearning/#method-__init__","title":"method <code>__init__</code>","text":"<pre><code>__init__(image, endmembers, nodata=-99999)\n</code></pre> <p>Parameter: </p> <p>image: Optical images. It must be rasterio.io.DatasetReader with 3d.  </p> <p>endmembers: Endmembers must be a matrix (numpy.ndarray) and with more than one endmember.   Rows represent the endmembers and columns represent the spectral bands.  The number of bands must be equal to the number of endmembers.  E.g. an image with 6 bands, endmembers dimension should be $n*6$, where $n$   is rows with the number of endmembers and 6 is the number of bands   (should be equal).  In addition, Endmembers must have a field (type int or float) with the names   of classes to be predicted.  </p> <p>nodata: The NoData value to replace with -99999.  </p> <p></p>"},{"location":"deeplearning/#method-fullyconnected","title":"method <code>FullyConnected</code>","text":"<pre><code>FullyConnected(\n    hidden_layers=3,\n    hidden_units=[64, 32, 16],\n    output_units=10,\n    input_shape=(6,),\n    epochs=300,\n    batch_size=32,\n    training_split=0.8,\n    random_state=None\n)\n</code></pre> <p>This algorithm consiste of a network with a sequence of Dense layers, which area densely  connnected (also called fully connected) neural layers. This is the simplest of deep  learning. </p> <p>Parameters:</p> <ul> <li> <p><code>hidden_layers</code>:  Number of hidden layers to be used. 3 is for default. </p> </li> <li> <p><code>hidden_units</code>:  Number of units to be used. This is related to 'neurons' in each hidden   layers.  </p> </li> <li> <p><code>output_units</code>:  Number of clases to be obtained. </p> </li> <li> <p><code>input_shape</code>:  The input shape is generally the shape of the input data provided to the   Keras model while training. The model cannot know the shape of the   training data. The shape of other tensors(layers) is computed automatically. </p> </li> <li> <p><code>epochs</code>:  Number of iteration, the network will compute the gradients of the weights with  regard to the loss on the batch, and update the weights accordingly. </p> </li> <li> <p><code>batch_size</code>:  This break the data into small batches. In deep learning, models do not   process antire dataset at once. </p> </li> <li> <p><code>training_split</code>:  For splitting samples into two subsets, i.e. training data and for testing  data. </p> </li> <li> <p><code>random_state</code>:  Random state ensures that the splits that you generate are reproducible.  </p> </li> <li><code>Please, see for more details https</code>: //scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html </li> </ul> <p>Return: </p> <p>A dictionary with Labels of classification as numpy object, overall accuracy,  among others results. </p> <p>This file was automatically generated via lazydocs.</p>"},{"location":"faq/","title":"FAQ","text":""},{"location":"faq/#how-do-i-report-an-issue-or-make-a-feature-request","title":"How do I report an issue or make a feature request:","text":"<p>Please go to https://github.com/ytarazona/scikit-eo/issues.</p>"},{"location":"faq/#1-why-machine-learning-algorithms-not-running","title":"1. Why machine learning algorithms not running?","text":"<p>Probably due to your dataset has NaNs values.</p>"},{"location":"faq/#2-why-is-the-image-crop-tool-not-cutting","title":"2. Why is the image crop tool not cutting?","text":"<p>The image crop tool <code>crop()</code> need two inputs: a raster image and a shapefile vector. Both must be in the same projection befores using the tool.</p>"},{"location":"faq/#3-where-can-i-find-the-references-related-to-the-functionsclasses","title":"3. Where can I find the references related to the functions/classes?","text":"<p>Each function/classes has a reference. This can be found inside the function.</p>"},{"location":"fusionrs/","title":"fusionrs module","text":""},{"location":"fusionrs/#module-fusionrs","title":"module <code>fusionrs</code>","text":""},{"location":"fusionrs/#function-fusionrs","title":"function <code>fusionrs</code>","text":"<pre><code>fusionrs(optical, radar, stand_varb=True, nodata=-99999, **kwargs)\n</code></pre> <p>Fusion of images with different observation geometries through Principal Component Analysis (PCA). </p> <p>This algorithm allows to fusion images coming from different spectral sensors  (e.g., optical-optical, optical and SAR, or SAR-SAR). It is also possible to obtain the contribution (%) of each variable in the fused image. </p> <p>Parameters:</p> <ul> <li> <p><code>optical</code>:  Optical image. It must be rasterio.io.DatasetReader with 3d. </p> </li> <li> <p><code>radar</code>:  Radar image. It must be rasterio.io.DatasetReader with 3d. </p> </li> <li> <p><code>stand_varb</code>:  Logical. If <code>stand.varb = True</code>, the PCA is calculated using the correlation   matrix (standardized variables) instead of the covariance matrix   (non-standardized variables).  </p> </li> <li> <p><code>nodata</code>:  The NoData value to replace with -99999. </p> </li> <li> <p><code>**kwargs</code>:  These will be passed to scikit-learn PCA, please see full lists at: </p> </li> <li><code>https</code>: //scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html </li> </ul> <p>Return: </p> <p>A dictionary. </p> <p>References: </p> <p>Tarazona, Y., Zabala, A., Pons, X., Broquetas, A., Nowosad, J., and Zurqani, H.A.  Fusing Landsat and SAR data for mapping tropical deforestation through machine learning  classification and the PVts-\u03b2 non-seasonal detection approach, Canadian Journal of Remote  Sensing., vol. 47, no. 5, pp. 677\u2013696, Sep. 2021. </p> <p>Note:</p> <p>Before executing the function, it is recommended that images coming from different sensors or from the same sensor have a co-registration.  The contributions of variables in accounting for the variability in a given principal component are expressed in percentage. Variables that are correlated with PC1 (i.e., Dim.1) and PC2 (i.e., Dim.2) are the most important in explaining the variability in the data set. Variables that do not correlated with any PC or correlated with the last dimensions are variables with low contribution and might be removed to simplify the overall analysis. The contribution is a scaled version of the squared correlation between variables and component axes (or the cosine, from a geometrical point of view) --- this is used to assess the quality of the representation of the variables of the principal component, and it is computed as (cos(variable,axis)^2/total cos2 of the component)\u00d7100. </p> <p>This file was automatically generated via lazydocs.</p>"},{"location":"get-started/","title":"Get Started","text":""},{"location":"get-started/#get-started","title":"Get Started","text":"<p>This Get Started is intended as a guide to apply several remote sensing tools in order to analyze and process satellite imagery such as Landsat, Sentinel-2, etc. Various methods including ML/DL, Spectral Mixture Analysis, Calibrations methods, Principal Component Analysis, among others are available in this python package. </p>"},{"location":"get-started/#content","title":"Content","text":"<p>Example 01: Random Forest (RF) classifier</p> <p>Example 02: Calibration methods for supervised classification</p> <p>Example 03: Imagery Fusion - optical/radar</p> <p>Example 04: Confusion matrix by estimated proportions of area with a confidence interval at 95%</p>"},{"location":"get-started/#brief-examples","title":"Brief examples","text":""},{"location":"get-started/#example-01-random-forest-rf-classifier","title":"Example 01: Random Forest (RF) classifier","text":"<p>In this example, in a small region of southern Brazil, optical imagery from Landsat-8 OLI (Operational Land Imager) will be used to classify land cover using the machine learning algorithm Random Forest (RF) (Breiman, 2001). Four types of land cover will be mapped, i.e., agriculture, forest, bare soil and water. The input data needed is the satellite image and the spectral signatures collected. The output as a dictionary will provide: i) confusion matrix, ii) overall accuracy, iii) kappa index and iv) a classes map.</p>"},{"location":"get-started/#01-optical-image-to-be-used","title":"01. Optical image to be used","text":"<p>Landsat-8 OLI (Operational Land Imager) will be used to obtain in order to classify using Random Forest (RF). This image, which is in surface reflectance with bands:</p> <ul> <li>Blue -&gt; B2</li> <li>Green -&gt; B3 </li> <li>Red -&gt; B4</li> <li>Nir -&gt; B5</li> <li>Swir1 -&gt; B6</li> <li>Swir2 -&gt; B7</li> </ul> <p>The image and signatures to be used can be downloaded here:</p>"},{"location":"get-started/#02-libraries-to-be-used-in-this-example","title":"02. Libraries to be used in this example","text":"<pre><code>import rasterio\nimport numpy as np\nfrom scikeo.mla import MLA\nimport matplotlib.pyplot as plt\nfrom dbfread import DBF\nimport matplotlib as mpl\nimport pandas as pd\n</code></pre>"},{"location":"get-started/#03-image-and-endmembers","title":"03. Image and endmembers","text":"<p>We will upload a satellite image as a .tif and endmembers as a .dbf.</p> <pre><code>path_raster = r\"C:\\data\\ml\\LC08_232066_20190727_SR.tif\"\nimg = rasterio.open(path_raster)\n\npath_endm = r\"C:\\data\\ml\\endmembers.dbf\"\nendm = DBF(path_endm)\n\n# endmembers\ndf = pd.DataFrame(iter(endm))\ndf.head()\n</code></pre> <p> </p>"},{"location":"get-started/#04-classifying-with-random-forest","title":"04. Classifying with Random Forest","text":"<p>An instance of <code>mla()</code>:</p> <p><pre><code>inst = MLA(image = img, endmembers = endm)\n</code></pre> Applying with 70% of data to train:</p> <pre><code>rf_class = inst.SVM(training_split = 0.7)\n</code></pre>"},{"location":"get-started/#50-results","title":"5.0 Results","text":"<p>Dictionary of results</p> <p><pre><code>rf_class.keys()\n</code></pre> Overall accuracy</p> <p><pre><code>rf_class.get('Overall_Accuracy')\n</code></pre> Kappa index</p> <p><pre><code>rf_class.get('Kappa_Index')\n</code></pre> Confusion matrix or error matrix</p> <pre><code>rf_class.get('Confusion_Matrix')\n</code></pre> <p> </p>"},{"location":"get-started/#06-preparing-the-image-before-plotting","title":"06. Preparing the image before plotting","text":"<p><pre><code># Let's define the color palette\npalette = mpl.colors.ListedColormap([\"#2232F9\",\"#F922AE\",\"#229954\",\"#7CED5E\"])\n</code></pre> Applying the <code>plotRGB()</code> algorithm is easy:</p> <pre><code># Let\u00b4s plot\nfig, axes = plt.subplots(nrows = 1, ncols = 2, figsize = (15, 9))\n\n# satellite image\nplotRGB(img, title = 'Image in Surface Reflectance', ax = axes[0])\n\n# class results\naxes[1].imshow(svm_class.get('Classification_Map'), cmap = palette)\naxes[1].set_title(\"Classification map\")\naxes[1].grid(False)\n</code></pre> <p> </p>"},{"location":"get-started/#example-02-calibration-methods-for-supervised-classification","title":"Example 02: Calibration methods for supervised classification","text":"<p>Given a large number of machine learning algorithms, it is necessary to select the one with the best performance in the classification, i.e., the algorithm in which the training and testing data used converge the learning iteratively to a solution that appears to be satisfactory (Tarazona et al., 2021). To deal with this, users can apply the calibration methods Leave One Out Cross-Validation (LOOCV), Cross-Validation (CV) and Monte Carlo Cross-Validation (MCCV) in order to calibrate a supervised classification with different algorithms. The input data needed are the spectral signatures collected as a .dbf or .csv. The output will provide a graph with the errors of each classifier obtained.</p>"},{"location":"get-started/#01-endmembers-as-a-dbf","title":"01. Endmembers as a .dbf","text":"<pre><code>path_endm = \"\\data\\ex_O2\\\\endmembers\\endmembers.dbf\"\nendm = DBF(path_endm)\n</code></pre>"},{"location":"get-started/#02-an-instance-of-calmla","title":"02. An instance of calmla()","text":"<pre><code>inst = calmla(endmembers = endm)\n</code></pre>"},{"location":"get-started/#03-applying-the-splitdata-method","title":"03. Applying the splitData() method","text":"<p><pre><code>data = inst.splitData()\n</code></pre> Calibrating with Monte Carlo Cross-Validation Calibration (MCCV)</p> <p>Parameters:</p> <ul> <li><code>split_data</code>: An instance obtaind with <code>splitData()</code>.</li> <li><code>models</code>: Support Vector Machine (svm), Decision Tree (dt), Random Forest (rf) and Naive Bayes (nb).</li> <li><code>n_iter</code>: Number of iterations.</li> </ul>"},{"location":"get-started/#04-running-mccv","title":"04. Running MCCV","text":"<pre><code>error_mccv = inst.MCCV(split_data = data, models = ('svm', 'dt', 'rf', 'nb'), \n                       n_iter = 10)\n</code></pre> <p>Calibration results:</p> <p></p> <p>With this result it can be observed that SVM and RF obtained a higher overall accuracy (less error). Therefore, you can use these algorithms to classify a satellite image.</p>"},{"location":"get-started/#example-03-imagery-fusion-opticalradar","title":"Example 03: Imagery Fusion - optical/radar","text":"<p>This is an area where scikit-eo provides a novel approach to merge different types of satellite imagery. We are in a case where, after combining different variables into a single output, we want to know the contributions of the different original variables in the data fusion. The fusion of radar and optical images, despite of its well-know use, to improve land cover mapping, currently has no tools that help researchers to integrate or combine those resources. In this third example, users can apply imagery fusion with different observation geometries and different ranges of the electromagnetic spectrum (Tarazona et al., 2021). The input data needed are the optical satellite image and the radar satellite image, for instance.</p> <p>In <code>scikit-eo</code> we developed the function <code>fusionrs()</code> which provides us with a dictionary with the following image fusion interpretation features:</p> <ul> <li>Fused_images: The fusion of both images into a 3-dimensional array (rows, cols, bands).</li> <li>Variance: The variance obtained.</li> <li>Proportion_of_variance: The proportion of the obtained variance.</li> <li>Cumulative_variance: The cumulative variance.</li> <li>Correlation: Correlation of the original bands with the principal components.</li> <li>Contributions_in_%: The contributions of each optical and radar band in the fusion.</li> </ul>"},{"location":"get-started/#01-loagind-dataset","title":"01. Loagind dataset","text":"<p>Loading a radar and optical imagery with a total of 9 bands. Optical imagery has 6 bands Blue, Green, Red, NIR, SWIR1 and SWIR2, while radar imagery has 3 bandas VV, VH and VV/VH.</p> <pre><code>path_optical = \"data/ex_03/LC08_003069_20180906.tif\"\noptical = rasterio.open(path_optical)\n\npath_radar = \"data/ex_03/S1_2018_VV_VH.tif\"\nradar = rasterio.open(path_radar)\n</code></pre>"},{"location":"get-started/#02-applying-the-fusionrs","title":"02. Applying the fusionrs:","text":"<pre><code>fusion = fusionrs(optical = optical, radar = radar)\n</code></pre>"},{"location":"get-started/#03-dictionary-of-results","title":"03. Dictionary of results:","text":"<pre><code>fusion.keys()\n</code></pre>"},{"location":"get-started/#04-proportion-of-variance","title":"04. Proportion of variance:","text":"<pre><code>prop_var = fusion.get('Proportion_of_variance')\n</code></pre>"},{"location":"get-started/#05-cumulative-variance","title":"05. Cumulative variance (%):","text":"<pre><code>cum_var = fusion.get('Cumulative_variance')*100\n</code></pre>"},{"location":"get-started/#06-showing-the-proportion-of-variance-and-cumulative","title":"06. Showing the proportion of variance and cumulative:","text":"<pre><code>x_labels = ['PC{}'.format(i+1) for i in range(len(prop_var))]\n\nfig, axes = plt.subplots(figsize = (6,5))\nln1 = axes.plot(x_labels, prop_var, marker ='o', markersize = 6,  \n                label = 'Proportion of variance')\n\naxes2 = axes.twinx()\nln2 = axes2.plot(x_labels, cum_var, marker = 'o', color = 'r', \n                 label = \"Cumulative variance\")\n\nln = ln1 + ln2\nlabs = [l.get_label() for l in ln]\n\naxes.legend(ln, labs, loc = 'center right')\naxes.set_xlabel(\"Principal Component\")\naxes.set_ylabel(\"Proportion of Variance\")\naxes2.set_ylabel(\"Cumulative (%)\")\naxes2.grid(False)\nplt.show()\n</code></pre>"},{"location":"get-started/#07-contributions-of-each-variable-in","title":"07. Contributions of each variable in %:","text":"<pre><code>fusion.get('Contributions_in_%')\n</code></pre> <p>Here, var1, var2, ... var12 refer to Blue, Green, ... VV/VH bands respectively. It can be observed that var2 (Green) has a higher contribution percentage 16.9% than other variables. With respect to radar polarizaciones, we can note that var8 (VH polarization) has a higher contribution 11.8% than other radar bands.</p>"},{"location":"get-started/#08-preparing-the-image","title":"08. Preparing the image:","text":"<pre><code>arr = fusion.get('Fused_images')\n\n## Let\u00b4s plot\nfig, axes = plt.subplots(figsize = (8, 8))\nplotRGB(arr, bands = [1,2,3], title = 'Fusion of optical and radar images')\nplt.show()\n</code></pre>"},{"location":"get-started/#example-04-confusion-matrix-by-estimated-proportions-of-area-with-a-confidence-interval-at-95","title":"Example 04: Confusion matrix by estimated proportions of area with a confidence interval at 95%","text":"<p>In this final example, after obtaining the predicted class map, we are in a case where we want to know the uncertainties of each class. The assessing accuracy and area estimate will be obtained following guidance proposed by (Olofsson et al., 2014). All that users need are the confusion matrix and a previously obtained predicted class map.</p> <p><code>confintervalML</code> requires the following parameters:</p> <ul> <li>matrix: confusion matrix or error matrix in numpy.ndarray.</li> <li>image_pred: a 2-dimensional array (rows, cols). This array should be the classified image with predicted classes.</li> <li>pixel_size: Pixel size of the classified image. Set by default as 10 meters. In this example is 30 meters (Landsat).</li> <li>conf: Confidence interval. By default is 95% (1.96).</li> <li>nodata: No data must be specified as 0, NaN or any other value. Keep in mind with this parameter.</li> </ul> <pre><code>#### 01. Load raster data\npath_raster = r\"\\data\\ex_O4\\ml\\predicted_map.tif\"\nimg = rasterio.open(path_optical).read(1)\n\n#### 02. Load confusion matrix as .csv\npath_cm = r\"\\data\\ex_O4\\ml\\confusion_matrix.csv\"\nvalues = pd.read_csv(path_radar)\n\n#### 03. Applying the confintervalML:\nconfintervalML(matrix = values, image_pred = img, pixel_size = 30, conf = 1.96, \n               nodata = -9999)\n</code></pre> <p>Results:</p> <p></p>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#installation","title":"Installation","text":"<p>To use scikit-eo it is necessary to install it in your terminal. There are two options to use its functions/classes:</p>"},{"location":"installation/#1-from-pypi","title":"1. From PyPI","text":"<p>scikit-eo is available on PyPI, so to install it, run this command in your terminal:</p> <pre><code>pip install scikeo\n</code></pre> <p>If you don't have pip installed, this Python installation guide can guide you through the process.</p>"},{"location":"installation/#2-installing-from-source","title":"2. Installing from source","text":"<p>It is also possible to install the latest development version directly from the GitHub repository with:</p> <p><pre><code>pip install git+https://github.com/ytarazona/scikit-eo\n</code></pre> This is the preferred method to install scikit-eo, as it will always install the most recent stable release.</p>"},{"location":"linearTrend/","title":"linearTrend module","text":""},{"location":"linearTrend/#module-lineartrend","title":"module <code>linearTrend</code>","text":""},{"location":"linearTrend/#class-lineartrend","title":"class <code>linearTrend</code>","text":"<p>Linear Trend in Remote Sensing </p> <p></p>"},{"location":"linearTrend/#method-__init__","title":"method <code>__init__</code>","text":"<pre><code>__init__(image, nodata=-99999)\n</code></pre> <p>Parameters:</p> <ul> <li> <p><code>image</code>:  Optical images. It must be rasterio.io.DatasetReader with 3d. </p> </li> <li> <p><code>nodata</code>:  The NoData value to replace with -99999. </p> </li> </ul> <p></p>"},{"location":"linearTrend/#method-ln","title":"method <code>LN</code>","text":"<pre><code>LN(**kwargs)\n</code></pre> <p>Linear trend is useful for mapping forest degradation, land degradation, etc. This algorithm is capable of obtaining the slope of an ordinary least-squares  linear regression and its reliability (p-value). </p> <p>Parameters:</p> <ul> <li><code>**kwargs</code>:  These will be passed to LN, please see full lists at: </li> <li><code>https</code>: //docs.scipy.org/doc/scipy/reference/generated/scipy.stats.linregress.html </li> </ul> <p>Return: a dictionary with slope, intercept and p-value obtained. All of them in numpy.ndarray  with 2d. </p> <p>References: </p> <p>Tarazona, Y., Maria, Miyasiro-Lopez. (2020). Monitoring tropical forest degradation using remote sensing. Challenges and opportunities in the Madre de Dios region, Peru. Remote   - <code>Sensing Applications</code>:  Society and Environment, 19, 100337. </p> <p>Wilkinson, G.N., Rogers, C.E., 1973. Symbolic descriptions of factorial models for analysis of variance. Appl. Stat. 22, 392-399. </p> <p>Chambers, J.M., 1992. Statistical Models in S. CRS Press. </p> <p>Note:</p> <p>Linear regression is widely used to analyze forest degradation or land degradation. Specifically, the slope and its reliability are used as main parameters and they can be obtained with this function. On the other hand, logistic regression allows obtaining a degradation risk map, in other words, it is a probability map. </p> <p></p>"},{"location":"linearTrend/#method-lr","title":"method <code>LR</code>","text":"<pre><code>LR(col_pos=0, **kwargs)\n</code></pre> <p>Logistic Regression is a statistical analysis technique that can measure  statistically the relative influence of several factors and explain objectively how values  depend on predictor variables. This method is applied to remotely sensed data. </p> <p>Parameters:</p> <ul> <li><code>**kwargs</code>:  These will be passed to MLN, please see full lists at: </li> <li><code>https</code>: //www.statsmodels.org/dev/generated/statsmodels.discrete.discrete_model.Logit.html </li> </ul> <p>Return: a dictionary with the summary of logistic regression and an array of probability with 2d. </p> <p>References: Tarazona, Y., Maria, Miyasiro-Lopez. (2020). Monitoring tropical forest degradation using remote sensing. Challenges and opportunities in the Madre de Dios region, Peru. Remote   - <code>Sensing Applications</code>:  Society and Environment, 19, 100337. </p> <p>Chambers, J.M., 1992. Statistical Models in S. CRS Press. </p> <p>Note:</p> <p>Logistic regression allows obtaining a degradation risk map (for instance), in other words, it is a probability map. </p> <p>This file was automatically generated via lazydocs.</p>"},{"location":"mla/","title":"mla module","text":""},{"location":"mla/#module-mla","title":"module <code>mla</code>","text":""},{"location":"mla/#class-mla","title":"class <code>MLA</code>","text":"<p>Supervised classification in Remote Sensing </p> <p></p>"},{"location":"mla/#method-__init__","title":"method <code>__init__</code>","text":"<pre><code>__init__(image, endmembers, nodata=-99999)\n</code></pre> <p>Parameter: </p> <p>image: Optical images. It must be rasterio.io.DatasetReader with 3d.  </p> <p>endmembers: Endmembers must be a matrix (numpy.ndarray) and with more than one endmember.   Rows represent the endmembers and columns represent the spectral bands.  The number of bands must be equal to the number of endmembers.  E.g. an image with 6 bands, endmembers dimension should be $n*6$, where $n$   is rows with the number of endmembers and 6 is the number of bands   (should be equal).  In addition, Endmembers must have a field (type int or float) with the names   of classes to be predicted.  </p> <p>nodata: The NoData value to replace with -99999.  </p> <p></p>"},{"location":"mla/#method-dt","title":"method <code>DT</code>","text":"<pre><code>DT(training_split=0.8, random_state=None, **kwargs)\n</code></pre> <p>Decision Tree is also a supervised non-parametric statistical learning technique, where the input data is divided recursively  into branches depending on certain decision thresholds until the data are segmented into homogeneous subgroups.  This technique has substantial advantages for remote sensing classification problems due to its flexibility, intuitive simplicity,  and computational efficiency. </p> <p>DT support raster data read by rasterio (rasterio.io.DatasetReader) as input. </p> <p>Parameters:</p> <ul> <li> <p><code>training_split</code>:  For splitting samples into two subsets, i.e. training data and for testing  data.  </p> </li> <li> <p><code>**kwargs</code>:  These will be passed to DT, please see full lists at: </p> </li> <li><code>https</code>: //scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier </li> </ul> <p>Return: </p> <p>A dictionary containing labels of classification as numpy object, overall accuracy, kappa index, confusion matrix. </p> <p></p>"},{"location":"mla/#method-nb","title":"method <code>NB</code>","text":"<pre><code>NB(training_split=0.8, random_state=None, **kwargs)\n</code></pre> <p>Naive Bayes classifier is an effective and simple method for image classification based on probability theory. The NB  classifier assumes an underlying probabilistic model and captures the uncertainty about the model in a principled way,  that is, by calculating the occurrence probabilities of different attribute values for different classes in a training  set. </p> <p>NB support raster data read by rasterio (rasterio.io.DatasetReader) as input. </p> <p>Parameters:</p> <ul> <li> <p><code>training_split</code>:  For splitting samples into two subsets, i.e. training data and for testing  data.  </p> </li> <li> <p><code>**kwargs</code>:  These will be passed to SVM, please see full lists at: </p> </li> <li><code>https</code>: //scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html </li> </ul> <p>Return: </p> <p>A dictionary containing labels of classification as numpy object, overall accuracy, kappa index, confusion matrix. </p> <p></p>"},{"location":"mla/#method-nn","title":"method <code>NN</code>","text":"<pre><code>NN(training_split=0.8, max_iter=300, random_state=None, **kwargs)\n</code></pre> <p>This classification consists of a neural network that is organized into several layers, that is, an input layer of predictor  variables, one or more layers of hidden nodes, in which each node represents an activation function acting on a weighted input  of the previous layers\u2019 outputs, and an output layer. </p> <p>NN support raster data read by rasterio (rasterio.io.DatasetReader) as input. </p> <p>Parameters:</p> <ul> <li> <p><code>training_split</code>:  For splitting samples into two subsets, i.e. training data and for testing  data.  </p> </li> <li> <p><code>**kwargs</code>:  These will be passed to SVM, please see full lists at: </p> </li> <li><code>https</code>: //scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html </li> </ul> <p>Return: </p> <p>A dictionary containing labels of classification as numpy object, overall accuracy, kappa index, confusion matrix. </p> <p></p>"},{"location":"mla/#method-rf","title":"method <code>RF</code>","text":"<pre><code>RF(training_split=0.8, random_state=None, **kwargs)\n</code></pre> <p>Random Forest is a derivative of Decision Tree which provides an improvement over DT to overcome the weaknesses of a single DT.  The prediction model of the RF classifier only requires two parameters to be identified: the number of classification trees desired,  known as \u201cntree,\u201d and the number of prediction variables, known as \u201cmtry,\u201d used in each node to make the tree grow. </p> <p>RF support raster data read by rasterio (rasterio.io.DatasetReader) as input. </p> <p>Parameters:</p> <ul> <li> <p><code>training_split</code>:  For splitting samples into two subsets, i.e. training data and for testing  data.  </p> </li> <li> <p><code>**kwargs</code>:  These will be passed to RF, please see full lists at: </p> </li> <li><code>https</code>: //scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html </li> </ul> <p>Return: </p> <p>A dictionary containing labels of classification as numpy object, overall accuracy, kappa index, confusion matrix. </p> <p></p>"},{"location":"mla/#method-svm","title":"method <code>SVM</code>","text":"<pre><code>SVM(training_split=0.8, random_state=None, kernel='linear', **kwargs)\n</code></pre> <p>The Support Vector Machine (SVM) classifier is a supervised non-parametric statistical learning technique that  does not assume a preliminary distribution of input data. Its discrimination criterion is a  hyperplane that separates the classes in the multidimensional space in which the samples  that have established the same classes are located, generally some training areas. </p> <p>SVM support raster data read by rasterio (rasterio.io.DatasetReader) as input. </p> <p>Parameters:</p> <ul> <li> <p><code>training_split</code>:  For splitting samples into two subsets, i.e. training data and for testing  data. </p> </li> <li> <p><code>kernel</code>:  {'linear', 'poly', 'rbf', 'sigmoid', 'precomputed'}, default='rbf' Specifies   the kernel type to be used in the algorithm. It must be one of 'linear', 'poly',   'rbf', 'sigmoid', 'precomputed' or a callable. If None is given, 'rbf' will  </p> </li> <li> <p><code>be used. See https</code>: //scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC for more details. </p> </li> <li> <p><code>**kwargs</code>:  These will be passed to SVM, please see full lists at: </p> </li> <li><code>https</code>: //scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC </li> </ul> <p>Return: </p> <p>A dictionary containing labels of classification as numpy object, overall accuracy, kappa index, confusion matrix. </p> <p>This file was automatically generated via lazydocs.</p>"},{"location":"pca/","title":"pca module","text":""},{"location":"pca/#module-pca","title":"module <code>pca</code>","text":""},{"location":"pca/#function-pca","title":"function <code>PCA</code>","text":"<pre><code>PCA(image, stand_varb=True, nodata=-99999, **kwargs)\n</code></pre> <p>Runing Principal Component Analysis (PCA) with satellite images. </p> <p>This algorithm allows to obtain Principal Components from images either radar or optical coming from different spectral sensors. It is also possible to obtain the contribution (%)  of each variable. </p> <p>Parameters:</p> <ul> <li> <p><code>images</code>:  Optical or radar image, it must be rasterio.io.DatasetReader with 3d. </p> </li> <li> <p><code>stand_varb</code>:  Logical. If <code>stand.varb = True</code>, the PCA is calculated using the correlation   matrix (standardized variables) instead of the covariance matrix   (non-standardized variables).  </p> </li> <li> <p><code>nodata</code>:  The NoData value to replace with -99999. </p> </li> <li> <p><code>**kwargs</code>:  These will be passed to scikit-learn PCA, please see full lists at: </p> </li> <li><code>https</code>: //scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html </li> </ul> <p>Return: </p> <p>A dictionary. </p> <p>Note:</p> <p>The contributions of variables in accounting for the variability in a given principal component are expressed in percentage. Variables that are correlated with PC1 (i.e., Dim.1) and PC2 (i.e., Dim.2) are the most important in explaining the variability in the data set. Variables that do not correlated with any PC or correlated with the last dimensions are variables with low contribution and might be removed to simplify the overall analysis. The contribution is a scaled version of the squared correlation between variables and component axes (or the cosine, from a geometrical point of view) --- this is used to assess the quality of the representation of the variables of the principal component, and it is computed as (cos(variable,axis)^2/total cos2 of the component)\u00d7100. </p> <p>This file was automatically generated via lazydocs.</p>"},{"location":"plot/","title":"plot module","text":""},{"location":"plot/#module-plot","title":"module <code>plot</code>","text":""},{"location":"plot/#function-plothist","title":"function <code>plotHist</code>","text":"<pre><code>plotHist(\n    image,\n    bands=1,\n    bins=128,\n    alpha=0.8,\n    title=None,\n    xlabel=None,\n    ylabel=None,\n    label=None,\n    ax=None,\n    density=True,\n    **kwargs\n)\n</code></pre> <p>This function allows to plot satellite images histogram. </p> <p>Parameters:</p> <ul> <li> <p><code>image</code>:  Optical images. It must be rasterio.io.DatasetReader with 3d or 2d.  </p> </li> <li> <p><code>bands</code>:  Must be specified as a number of a list.  </p> </li> <li> <p><code>bins</code>:  By default is 128.   </p> </li> <li> <p><code>alpha</code>:  Percentage (%) of transparency between 0 and 1. 0 indicates 0% and 1 indicates  100%. By default is 80%.  </p> </li> <li> <p><code>title</code>:  Assigned title. </p> </li> <li> <p><code>xlabel</code>:  X axis title. </p> </li> <li> <p><code>ylabel</code>:  Y axis title.  </p> </li> <li> <p><code>label</code>:  Labeling the histogram. </p> </li> <li> <p><code>ax</code>:  current axes </p> </li> <li> <p><code>**kwargs</code>:  These will be passed to the matplotlib imshow(), please see full lists at: </p> </li> <li><code>https</code>: //matplotlib.org/stable/api/_as_gen/matplotlib.axes.Axes.hist.html </li> </ul> <p>Return:   - <code>ax</code>:  A histogram of an image. </p> <p></p>"},{"location":"plot/#function-plotrgb","title":"function <code>plotRGB</code>","text":"<pre><code>plotRGB(\n    image,\n    bands=[3, 2, 1],\n    stretch='std',\n    title=None,\n    xlabel=None,\n    ylabel=None,\n    ax=None,\n    **kwargs\n)\n</code></pre> <p>Plotting an image in RGB. </p> <p>This function allows to plot an satellite image in RGB channels.  </p> <p>Parameters:</p> <ul> <li> <p><code>image</code>:  Optical images. It must be rasterio.io.DatasetReader with 3d.  </p> </li> <li> <p><code>bands</code>:  A list contain the order of bands to be used in order to plot in RGB. For example,  for six bands (blue, green, red, nir, swir1 and swir2), number four (4) indicates   the swir1 band, number three (3) indicates the nir band and the number two (2) indicates  the red band.  </p> </li> <li> <p><code>stretch</code>:  Contrast enhancement using the histogram. There are two options here: i) using  standard deviation ('std') and ii) using percentiles ('per'). For default is 'std', which means  standard deviation.  </p> </li> <li> <p><code>title</code>:  Assigned title. </p> </li> <li> <p><code>xlabel</code>:  X axis title. </p> </li> <li> <p><code>ylabel</code>:  Y axis title. </p> </li> <li> <p><code>ax</code>:  current axes </p> </li> <li> <p><code>**kwargs</code>:  These will be passed to the matplotlib imshow(), please see full lists at: </p> </li> <li><code>https</code>: //matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.imshow.html </li> </ul> <p>Return:   - <code>ax</code>:  Graphic of an image in RGB. </p> <p>This file was automatically generated via lazydocs.</p>"},{"location":"process/","title":"process module","text":""},{"location":"process/#module-process","title":"module <code>process</code>","text":""},{"location":"process/#function-crop","title":"function <code>crop</code>","text":"<pre><code>crop(image, shp, filename=None, filepath=None)\n</code></pre> <p>This algorithm allows to clip a raster (.tif) including a satellite image using a shapefile. </p> <p>Parameters:</p> <ul> <li> <p><code>image</code>:  This parameter can be a string with the raster path (e.g., r'/home/image/b3.tif') or it can be a rasterio.io.DatasetReader type. </p> </li> <li> <p><code>shp</code>:  Vector file, tipically shapefile. </p> </li> <li> <p><code>filename</code>:  The image name to be saved. </p> </li> <li> <p><code>filepath</code>:  The path which the image will be stored. </p> </li> </ul> <p>Return: </p> <p>A raster in your filepath. </p> <p></p>"},{"location":"process/#function-extract","title":"function <code>extract</code>","text":"<pre><code>extract(image, shp)\n</code></pre> <p>This algorithm allows to extract raster values using a shapefile. </p> <p>Parameters:</p> <ul> <li> <p><code>image</code>:  Optical images. It must be rasterio.io.DatasetReader with 3d or 2d. </p> </li> <li> <p><code>shp</code>:  Vector file, tipically shapefile. </p> </li> </ul> <p>Return: </p> <p>A dataframe with raster values obtained. </p> <p>Note:</p> <p>This function is usually used to extract raster values to be used on machine learning algorithms. </p> <p></p>"},{"location":"process/#function-confintervalml","title":"function <code>confintervalML</code>","text":"<pre><code>confintervalML(matrix, image_pred, pixel_size=10, conf=1.96, nodata=None)\n</code></pre> <p>The error matrix is a simple cross-tabulation of the class labels allocated by the classification of the remotely  sensed data against the reference data for the sample sites. The error matrix organizes the acquired sample data  in a way that summarizes key results and aids the quantification of accuracy and area. The main diagonal of the error  matrix highlights correct classifications while the off-diagonal elements show omission and commission errors.  The cell entries and marginal values of the error matrix are fundamental to both accuracy assessment and area  estimation. The cell entries of the population error matrix and the parameters derived from it must be estimated  from a sample. This function shows how to obtain a confusion matrix by estimated proportions of area with a confidence interval at 95% (1.96). </p> <p>Parameters:</p> <ul> <li> <p><code>matrix</code>:  confusion matrix or error matrix in numpy.ndarray.  </p> </li> <li> <p><code>image_pred</code>:  Could be an array with 2d (rows, cols). This array should be the image classified   with predicted classes. Or, could be a list with number of pixels for each class.  </p> </li> <li> <p><code>pixel_size</code>:  Pixel size of the image classified. By default is 10m of Sentinel-2.  </p> </li> <li> <p><code>conf</code>:  Confidence interval. By default is 95%. </p> </li> </ul> <p>Return: </p> <p>Information of confusion matrix by proportions of area, overall accuracy, user's accuracy with confidence interval  and estimated area with confidence interval as well.  </p> <p>Reference: </p> <p>Olofsson, P., Foody, G.M., Herold, M., Stehman, S.V., Woodcock, C.E., and Wulder, M.A. 2014. \u201cGood practices  - <code>for estimating area and assessing accuracy of land change.\u201d Remote Sensing of Environment, Vol. 148</code>:  42\u201357.  - <code>doi</code>: https://doi.org/10.1016/j.rse.2014.02.015. </p> <p>Note:</p> <p>Columns and rows in a confusion matrix indicate reference and prediction respectively. </p> <p></p>"},{"location":"process/#function-print_info","title":"function <code>print_info</code>","text":"<pre><code>print_info(params)\n</code></pre> <p>Information: Confusion Matrix by Estimated Proportions of area an uncertainty </p> <p>Parameters:</p> <ul> <li><code>params</code>:  <code>confintervalML</code> result. See the function: https://github.com/ytarazona/scikit-eo/blob/main/scikeo/process.py </li> </ul> <p>Return: </p> <p>Information of confusion matrix by proportions of area, overall accuracy, user's accuracy with confidence interval  and estimated area with confidence interval as well. </p> <p>Note:</p> <p>This function was tested using ground-truth values obtained by Olofsson et al. (2014).  </p> <p>This file was automatically generated via lazydocs.</p>"},{"location":"rkmeans/","title":"rkmeans module","text":""},{"location":"rkmeans/#module-rkmeans","title":"module <code>rkmeans</code>","text":""},{"location":"rkmeans/#function-rkmeans","title":"function <code>rkmeans</code>","text":"<pre><code>rkmeans(image, k, nodata=-99999, **kwargs)\n</code></pre> <p>This function allows to classify satellite images using k-means </p> <p>In principle, this function allows to classify satellite images specifying a <code>k</code> value (clusters), however it is recommended to find the optimal value of <code>k</code> using the <code>calkmeans</code> function embedded in this package.  </p> <p>Parameters:</p> <ul> <li> <p><code>image</code>:  Optical images. It must be rasterio.io.DatasetReader with 3d. </p> </li> <li> <p><code>k</code>:  The number of clusters to be detected. </p> </li> <li> <p><code>nodata</code>:  The NoData value to replace with -99999.  </p> </li> <li> <p><code>**kwargs</code>:  These will be passed to scikit-learn KMeans, please see full lists at: </p> </li> <li><code>https</code>: //scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html </li> </ul> <p>Return: </p> <p>Labels of classification as numpy object with 2d. </p> <p>This file was automatically generated via lazydocs.</p>"},{"location":"sma/","title":"sma module","text":""},{"location":"sma/#module-sma","title":"module <code>sma</code>","text":""},{"location":"sma/#function-sma","title":"function <code>sma</code>","text":"<pre><code>sma(image, endmembers, nodata=-99999)\n</code></pre> <p>The SMA assumes that the energy received within the field of vision of the remote sensor  can be considered as the sum of the energies received from each dominant endmember.  This function addresses a Linear Mixing Model. </p> <p>A regression analysis is used to obtain the fractions. In least squares inversion algorithms,  the common objective is to estimate abundances that minimize the squared error between the  actual spectrum and the estimated spectrum. The values of the fractions will be between 0 and 1. </p> <p>Parameters:</p> <ul> <li> <p><code>image</code>:  Optical images. It must be rasterio.io.DatasetReader with 3d. </p> </li> <li> <p><code>endmembers</code>:  Endmembers must be a matrix (numpy.ndarray) and with more than one endmember.   Rows represent the endmembers and columns represent the spectral bands.  The number of bands must be greater than the number of endmembers.  E.g. an image with 6 bands, endmembers dimension should be $n*6$, where $n$   is rows with the number of endmembers and 6 is the number of bands   (should be equal).  </p> </li> <li> <p><code>nodata</code>:  The NoData value to replace with -99999. </p> </li> </ul> <p>Return: </p> <p>numpy.ndarray with 2d. </p> <p>References: </p> <p>Adams, J. B., Smith, M. O., &amp; Gillespie, A. R. (1993). Imaging spectroscopy: Interpretation based on spectral mixture analysis. In C. M. Pieters &amp; P.   - <code>Englert (Eds.), Remote geochemical analysis</code>:  Elements and mineralogical   - <code>composition. NY</code>:  Cambridge Univ. Press 145-166 pp. </p> <p>Shimabukuro, Y.E. and Smith, J., (1991). The least squares mixing models to generate fraction images derived from remote sensing multispectral data. IEEE Transactions on Geoscience and Remote Sensing, 29, pp. 16-21. </p> <p>Note:</p> <p>A regression analysis is used to obtain the fractions. In least squares inversion algorithms, the common objective is to estimate abundances that minimize the squared error between the actual spectrum and the estimated spectrum. The values of the fractions will be between 0 and 1. </p> <p>This file was automatically generated via lazydocs.</p>"},{"location":"tassCap/","title":"tassCap module","text":""},{"location":"tassCap/#module-tasscap","title":"module <code>tassCap</code>","text":""},{"location":"tassCap/#function-tasscap","title":"function <code>tassCap</code>","text":"<pre><code>tassCap(image, sat='Landsat8OLI', nodata=-99999, scale=None)\n</code></pre> <p>The Tasseled-Cap Transformation is a linear transformation method for various  remote sensing data. Not only can it perform volume data compression, but it can also provide parameters associated with the physical characteristics,  such as brightness, greenness and wetness indices. </p> <p>Parameters:</p> <ul> <li> <p><code>image</code>:  Optical images. It must be rasterio.io.DatasetReader with 3d. </p> </li> <li> <p><code>sat</code>:  Specify satellite and sensor type (Landsat5TM, Landsat7ETM or Landsat8OLI).  </p> </li> <li> <p><code>nodata</code>:  The NoData value to replace with -99999. </p> </li> <li> <p><code>scale</code>:  Conversion of coefficients values </p> </li> </ul> <p>Return: numpy.ndarray with 3d containing brightness, greenness and wetness indices. </p> <p>References: </p> <p>Crist, E.P., R. Laurin, and R.C. Cicone. 1986. Vegetation and soils information  contained in transformed Thematic Mapper data. Pages 1465-1470 Ref. ESA SP-254.  - <code>European Space Agency, Paris, France. http</code>: //www.ciesin.org/docs/005-419/005-419.html. </p> <p>Baig, M.H.A., Shuai, T., Tong, Q., 2014. Derivation of a tasseled cap transformation  based on Landsat 8 at-satellite reflectance. Remote Sensing Letters, 5(5), 423-431.  </p> <p>Li, B., Ti, C., Zhao, Y., Yan, X., 2016. Estimating Soil Moisture with Landsat Data  and Its Application in Extracting the Spatial Distribution of Winter Flooded Paddies.  Remote Sensing, 8(1), 38. </p> <p>Note:</p> <p>Currently implemented for satellites such as Landsat-4 TM, Landsat-5 TM, Landsat-7 ETM+, Landsat-8 OLI and Sentinel2. The input data must be in top of atmosphere reflectance (toa). Bands required as input must be ordered as:  Consider using the following satellite bands: ===============   ================================ Type of Sensor     Name of bands ===============   ================================ Landsat4TM         :blue, green, red, nir, swir1, swir2 Landsat5TM         :blue, green, red, nir, swir1, swir2 Landsat7ETM+       :blue, green, red, nir, swir1, swir2 Landsat8OLI        :blue, green, red, nir, swir1, swir2 Landsat8OLI-Li2016 :coastal, blue, green, red, nir, swir1, swir2 Sentinel2MSI       :coastal, blue, green, red, nir-1, mir-1, mir-2 </p> <p>This file was automatically generated via lazydocs.</p>"},{"location":"tutorials/","title":"Tutorials","text":""},{"location":"tutorials/#tutorials","title":"Tutorials","text":"<p>Scikit-eo provides a rich suite of algorithms specifically designed for environmental studies. These include statistical analysis, machine learning, deep learning, data fusion and spatial analysis. Researchers can leverage these tools to explore patterns, relationships, and trends within their datasets, to uncover complex land or forest degradation or mapping and classify the land cover, and generate insightful visualizations, among others tools.</p>"},{"location":"tutorials/#scikit-eo-tutorials-notebooks","title":"Scikit-eo tutorials notebooks","text":"<ol> <li>Machine Learning.ipynb</li> <li>Estimated area and uncertainty in Machine Learning.ipynb</li> <li>Calibrating supervised classification in Remote Sensing.ipynb</li> <li>Kmeans classification.ipynb</li> <li>Fusion of radar and optical images.ipynb</li> <li>Spectral Mixture Analysis.ipynb</li> <li>Principal Components Analysis.ipynb</li> <li>Tasseled-Cap Transformation.ipynb</li> <li>Linear trend analysis.ipynb</li> <li>Logistic regression in remote sensing.ipynb</li> <li>Atmospheric Correction.ipynb</li> <li>Plot an satellite image in RGB.ipynb</li> <li>Plot a satellite image histogram.ipynb</li> <li>Deep Learning Classification FullyConnected.ipynb</li> <li>Clipping an image.ipynb</li> </ol>"},{"location":"writeRaster/","title":"writeRaster module","text":""},{"location":"writeRaster/#module-writeraster","title":"module <code>writeRaster</code>","text":""},{"location":"writeRaster/#function-writeraster","title":"function <code>writeRaster</code>","text":"<pre><code>writeRaster(arr, image, filename=None, filepath=None, n=1)\n</code></pre> <p>This algorithm allows to save array images to raster format (.tif). </p> <p>Parameters:</p> <ul> <li> <p><code>arr</code>:  Array object with 2d (rows and cols) or 3d (rows, cols, bands). </p> </li> <li> <p><code>image</code>:  Optical images. It must be read by rasterio.open(). </p> </li> <li> <p><code>filename</code>:  The image name to be saved. </p> </li> <li> <p><code>filepath</code>:  The path which the image will be stored. </p> </li> <li> <p><code>n</code>:  Number of images to be saved. </p> </li> </ul> <p>Return: A raster in your filepath. </p> <p>This file was automatically generated via lazydocs.</p>"}]}